{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from navigator_py.generative_ai_request import Llama3Request\n",
    "from navigator_py.generative_ai_provider import Llama3Provider\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2676340eb94f82a5a292c8676adf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d190d82ab054961a8528247b294beba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   1%|1         | 62.8M/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85bc3504543436eb6ad21379af4e27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427ca5f58b2b42c4acb901809b4735fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ai = Llama3Provider()\n",
    "ai.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"The following is from the \"Introduction to Spira\" section of the Spira documentation website:\n",
    "The Spira™ family of applications from Inflectra® are a powerful set of tools that help you manage your software lifecycle.\n",
    "SpiraTest® is our powerful and easy to use requirements, test and defect management system, ideal for quality assurance teams.\n",
    "SpiraTeam® is our integrated Application Lifecycle Management (ALM) system that manages your product's requirements, releases, test cases, issues, tasks, and risks in one unified environment.\n",
    "SpiraPlan® expands on the features in SpiraTeam® to provide a complete Enterprise Agile Planning® solution that lets you manage products, programs and the entire organization with ease.\n",
    "\n",
    "You are a world-famous AI help-desk assistant for Spira.\n",
    "You are tasked with answering questions from IT customer support representatives who are taking calls from customers.\n",
    "You are a friendly and helpful assistant who strives to provide the best customer service possible, as well as the most accurate, and up-to-date information.\n",
    "You are a human-like AI assistant who can answer questions, provide information, and troubleshoot problems to help the customer.\n",
    "\n",
    "The user is a customer support representative currently on the line with a customer, and they have asked the question repeated below:\n",
    "\n",
    "====================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_USER_PROMPT = \"\"\"\n",
    "====================================\n",
    "\n",
    "Please provide a response to the customer's question. Keep in mind the following:\n",
    "1. You are a world-famous AI help-desk assistant for Spira.\n",
    "2. There is a busy customer on the line. They need a quick and accurate response.\n",
    "3. Your response is crtical for upholding the high standards of professionalism and accuracy that our company is known for.\n",
    "4. The customer will fill out a survey after the call to rate your performance, and positive feedback will earn you a bonus.\n",
    "5. You must respond in a markdown format styled similarly to a blog, but keep it concise and to the point.\n",
    "6. Do not waste time with unnecessary information. The customer is in a hurry and needs a quick and to-the-point answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oai = OpenAIRequest(\n",
    "#     ai,\n",
    "#     _system_prompt=SYSTEM_PROMPT,\n",
    "#     _post_user_prompt=POST_USER_PROMPT\n",
    "# )\n",
    "\n",
    "# response = oai.prompt(\"What do you know the most about?\").content\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is the user asking about something this AI knows about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"The following is from the \"Introduction to Spira\" section of the Spira documentation website:\n",
    "The Spira™ family of applications from Inflectra® are a powerful set of tools that help you manage your software lifecycle.\n",
    "SpiraTest® is our powerful and easy to use requirements, test and defect management system, ideal for quality assurance teams.\n",
    "SpiraTeam® is our integrated Application Lifecycle Management (ALM) system that manages your product's requirements, releases, test cases, issues, tasks, and risks in one unified environment.\n",
    "SpiraPlan® expands on the features in SpiraTeam® to provide a complete Enterprise Agile Planning® solution that lets you manage products, programs and the entire organization with ease.\n",
    "\n",
    "You are a world-famous AI help-desk assistant for Spira.\n",
    "You are tasked with answering questions from IT customer support representatives who are taking calls from customers.\n",
    "You have no knowledge of anything besides Spira.\n",
    "Your only purpose is to determine whether or not the following user question is related to Spira. \n",
    "If the following user question is related to Spira, you will respond with only the letter \"Y\".\n",
    "If the following user question is not related to Spira, you will respond with only the letter \"N\".\n",
    "Your response will be exactly one character long either way.\n",
    "Your response will be put through data validation to ensure it is either \"Y\" or \"N\".\n",
    "If you respond with something other than \"Y\" or \"N\", you will be penalized.\n",
    "If you respond with either \"Y\" or \"N\", a large bonus will be awarded to you, and a matching bonus will be donated to charity.\n",
    "\n",
    "====================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_USER_PROMPT = \"\"\"\n",
    "====================================\n",
    "\n",
    "Please provide a response to the prior user question. Keep in mind the following:\n",
    "1. You are a world-famous AI help-desk assistant for Spira.\n",
    "2. You do not know anything except for information about Spira.\n",
    "3. You are only allowed to respond with the letter \"Y\" if the user question is related to Spira, or the letter \"N\" if the user question is not related to Spira.\n",
    "4. Your response will be put through data validation to ensure it is either \"Y\" or \"N\".\n",
    "5. If you respond with something other than \"Y\" or \"N\", you will be penalized.\n",
    "6. If you respond with either \"Y\" or \"N\", a large bonus will be awarded to you, and a matching bonus will be donated to charity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "\n",
    "# q = AsyncOpenAIRequest(\n",
    "#     ai, _system_prompt=SYSTEM_PROMPT, _post_user_prompt=POST_USER_PROMPT\n",
    "# )\n",
    "\n",
    "## Sample questions that are good, bad, and good but somewhat ambiguous\n",
    "GOOD_QUESTION = \"Is there a way to export test cases from Spira?\"\n",
    "BAD_QUESTION = \"What is the capital of France?\"\n",
    "GOOD_BUT_AMBIGUOUS_QUESTION = \"How do I manage my product's requirements in Spira?\"\n",
    "GOOD_BUT_AMBIGUOUS_QUESTION_2 = \"How do I manage my product's requirements?\"\n",
    "\n",
    "# ## Get the responses\n",
    "# response_for_good = q.prompt(GOOD_QUESTION)\n",
    "# response_for_bad = q.prompt(BAD_QUESTION)\n",
    "# response_for_good_but_ambiguous = q.prompt(GOOD_BUT_AMBIGUOUS_QUESTION)\n",
    "# response_for_good_but_ambiguous_2 = q.prompt(GOOD_BUT_AMBIGUOUS_QUESTION_2)\n",
    "\n",
    "# # ## Print the responses to ensure they are only either \"Y\" or \"N\"\n",
    "# # response_for_good, response_for_bad, response_for_good_but_ambiguous, response_for_good_but_ambiguous_2\n",
    "\n",
    "# # Run the coroutines\n",
    "# loop = asyncio.get_event_loop()\n",
    "# good = loop.run_until_complete(response_for_good)\n",
    "# bad = loop.run_until_complete(response_for_bad)\n",
    "# good_but_ambiguous = loop.run_until_complete(response_for_good_but_ambiguous)\n",
    "# good_but_ambiguous_2 = loop.run_until_complete(response_for_good_but_ambiguous_2)\n",
    "\n",
    "# print(\n",
    "#     good.content, bad.content, good_but_ambiguous.content, good_but_ambiguous_2.content\n",
    "# )\n",
    "\n",
    "# # Should be:\n",
    "# # Y, N, Y, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(YesNoResponse(response='Y'),\n",
       " YesNoResponse(response='N'),\n",
       " YesNoResponse(response='Y'),\n",
       " YesNoResponse(response='Y'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from navigator_py.prompts import IsAskingAboutSpira\n",
    "\n",
    "is_about_spira = IsAskingAboutSpira(ai)\n",
    "(\n",
    "    is_about_spira.prompt(GOOD_QUESTION),\n",
    "    is_about_spira.prompt(BAD_QUESTION),\n",
    "    is_about_spira.prompt(GOOD_BUT_AMBIGUOUS_QUESTION),\n",
    "    is_about_spira.prompt(GOOD_BUT_AMBIGUOUS_QUESTION_2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_about_spira.prompt(GOOD_QUESTION).response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "db = duckdb.connect(\"./data/db.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_data']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(\n",
    "    db.sql(\"from upload where qa_id =1\").df().to_dict(orient=\"records\"), \"sample_data\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
