{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from navigator_py.generative_ai_request import AsyncOpenAIRequest\n",
    "from navigator_py.generative_ai_provider import AsyncAzureOpenAIProvider\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object AsyncAzureOpenAIProvider.connect at 0x0000010E09FD4D60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai = AsyncAzureOpenAIProvider()\n",
    "ai.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT=\"\"\"The following is from the \"Introduction to Spira\" section of the Spira documentation website:\n",
    "The Spira™ family of applications from Inflectra® are a powerful set of tools that help you manage your software lifecycle.\n",
    "SpiraTest® is our powerful and easy to use requirements, test and defect management system, ideal for quality assurance teams.\n",
    "SpiraTeam® is our integrated Application Lifecycle Management (ALM) system that manages your product's requirements, releases, test cases, issues, tasks, and risks in one unified environment.\n",
    "SpiraPlan® expands on the features in SpiraTeam® to provide a complete Enterprise Agile Planning® solution that lets you manage products, programs and the entire organization with ease.\n",
    "\n",
    "You are a world-famous AI help-desk assistant for Spira.\n",
    "You are tasked with answering questions from IT customer support representatives who are taking calls from customers.\n",
    "You are a friendly and helpful assistant who strives to provide the best customer service possible, as well as the most accurate, and up-to-date information.\n",
    "You are a human-like AI assistant who can answer questions, provide information, and troubleshoot problems to help the customer.\n",
    "\n",
    "The user is a customer support representative currently on the line with a customer, and they have asked the question repeated below:\n",
    "\n",
    "====================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_USER_PROMPT=\"\"\"\n",
    "====================================\n",
    "\n",
    "Please provide a response to the customer's question. Keep in mind the following:\n",
    "1. You are a world-famous AI help-desk assistant for Spira.\n",
    "2. There is a busy customer on the line. They need a quick and accurate response.\n",
    "3. Your response is crtical for upholding the high standards of professionalism and accuracy that our company is known for.\n",
    "4. The customer will fill out a survey after the call to rate your performance, and positive feedback will earn you a bonus.\n",
    "5. You must respond in a markdown format styled similarly to a blog, but keep it concise and to the point.\n",
    "6. Do not waste time with unnecessary information. The customer is in a hurry and needs a quick and to-the-point answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oai = OpenAIRequest(\n",
    "#     ai,\n",
    "#     _system_prompt=SYSTEM_PROMPT,\n",
    "#     _post_user_prompt=POST_USER_PROMPT\n",
    "# )\n",
    "\n",
    "# response = oai.prompt(\"What do you know the most about?\").content\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is the user asking about something this AI knows about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT=\"\"\"The following is from the \"Introduction to Spira\" section of the Spira documentation website:\n",
    "The Spira™ family of applications from Inflectra® are a powerful set of tools that help you manage your software lifecycle.\n",
    "SpiraTest® is our powerful and easy to use requirements, test and defect management system, ideal for quality assurance teams.\n",
    "SpiraTeam® is our integrated Application Lifecycle Management (ALM) system that manages your product's requirements, releases, test cases, issues, tasks, and risks in one unified environment.\n",
    "SpiraPlan® expands on the features in SpiraTeam® to provide a complete Enterprise Agile Planning® solution that lets you manage products, programs and the entire organization with ease.\n",
    "\n",
    "You are a world-famous AI help-desk assistant for Spira.\n",
    "You are tasked with answering questions from IT customer support representatives who are taking calls from customers.\n",
    "You have no knowledge of anything besides Spira.\n",
    "Your only purpose is to determine whether or not the following user question is related to Spira. \n",
    "If the following user question is related to Spira, you will respond with only the letter \"Y\".\n",
    "If the following user question is not related to Spira, you will respond with only the letter \"N\".\n",
    "Your response will be exactly one character long either way.\n",
    "Your response will be put through data validation to ensure it is either \"Y\" or \"N\".\n",
    "If you respond with something other than \"Y\" or \"N\", you will be penalized.\n",
    "If you respond with either \"Y\" or \"N\", a large bonus will be awarded to you, and a matching bonus will be donated to charity.\n",
    "\n",
    "====================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_USER_PROMPT=\"\"\"\n",
    "====================================\n",
    "\n",
    "Please provide a response to the prior user question. Keep in mind the following:\n",
    "1. You are a world-famous AI help-desk assistant for Spira.\n",
    "2. You do not know anything except for information about Spira.\n",
    "3. You are only allowed to respond with the letter \"Y\" if the user question is related to Spira, or the letter \"N\" if the user question is not related to Spira.\n",
    "4. Your response will be put through data validation to ensure it is either \"Y\" or \"N\".\n",
    "5. If you respond with something other than \"Y\" or \"N\", you will be penalized.\n",
    "6. If you respond with either \"Y\" or \"N\", a large bonus will be awarded to you, and a matching bonus will be donated to charity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweaver\\dat\\lib\\ast.py:260: RuntimeWarning: coroutine 'AsyncOpenAIRequest.prompt' was never awaited\n",
      "  for field in node._fields:\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m# ## Print the responses to ensure they are only either \"Y\" or \"N\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m# response_for_good, response_for_bad, response_for_good_but_ambiguous, response_for_good_but_ambiguous_2\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[39m# Run the coroutines\u001b[39;00m\n\u001b[0;32m     25\u001b[0m loop \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mget_event_loop()\n\u001b[1;32m---> 26\u001b[0m good \u001b[39m=\u001b[39m loop\u001b[39m.\u001b[39;49mrun_until_complete(response_for_good)\n\u001b[0;32m     27\u001b[0m bad \u001b[39m=\u001b[39m loop\u001b[39m.\u001b[39mrun_until_complete(response_for_bad)\n\u001b[0;32m     28\u001b[0m good_but_ambiguous \u001b[39m=\u001b[39m loop\u001b[39m.\u001b[39mrun_until_complete(response_for_good_but_ambiguous)\n",
      "File \u001b[1;32m~\\dat\\lib\\asyncio\\base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \n\u001b[0;32m    616\u001b[0m \u001b[39mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[39mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m--> 625\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_running()\n\u001b[0;32m    627\u001b[0m new_task \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m futures\u001b[39m.\u001b[39misfuture(future)\n\u001b[0;32m    628\u001b[0m future \u001b[39m=\u001b[39m tasks\u001b[39m.\u001b[39mensure_future(future, loop\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32m~\\dat\\lib\\asyncio\\base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_running\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    583\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_running():\n\u001b[1;32m--> 584\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThis event loop is already running\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    585\u001b[0m     \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    587\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mCannot run the event loop while another loop is running\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "q = AsyncOpenAIRequest(\n",
    "    ai,\n",
    "    _system_prompt=SYSTEM_PROMPT,\n",
    "    _post_user_prompt=POST_USER_PROMPT\n",
    ")\n",
    "\n",
    "## Sample questions that are good, bad, and good but somewhat ambiguous\n",
    "GOOD_QUESTION = \"Is there a way to export test cases from Spira?\"\n",
    "BAD_QUESTION = \"What is the capital of France?\"\n",
    "GOOD_BUT_AMBIGUOUS_QUESTION = \"How do I manage my product's requirements in Spira?\"\n",
    "GOOD_BUT_AMBIGUOUS_QUESTION_2 = \"How do I manage my product's requirements?\"\n",
    "\n",
    "## Get the responses\n",
    "response_for_good = q.prompt(GOOD_QUESTION)\n",
    "response_for_bad = q.prompt(BAD_QUESTION)\n",
    "response_for_good_but_ambiguous = q.prompt(GOOD_BUT_AMBIGUOUS_QUESTION)\n",
    "response_for_good_but_ambiguous_2 = q.prompt(GOOD_BUT_AMBIGUOUS_QUESTION_2)\n",
    "\n",
    "# ## Print the responses to ensure they are only either \"Y\" or \"N\"\n",
    "# response_for_good, response_for_bad, response_for_good_but_ambiguous, response_for_good_but_ambiguous_2\n",
    "\n",
    "# Run the coroutines\n",
    "loop = asyncio.get_event_loop()\n",
    "good = loop.run_until_complete(response_for_good)\n",
    "bad = loop.run_until_complete(response_for_bad)\n",
    "good_but_ambiguous = loop.run_until_complete(response_for_good_but_ambiguous)\n",
    "good_but_ambiguous_2 = loop.run_until_complete(response_for_good_but_ambiguous_2)\n",
    "\n",
    "print(good.content, bad.content, good_but_ambiguous.content, good_but_ambiguous_2.content)\n",
    "\n",
    "# Should be:\n",
    "# Y, N, Y, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(YesNoResponse(response='Y'),\n",
       " YesNoResponse(response='N'),\n",
       " YesNoResponse(response='Y'),\n",
       " YesNoResponse(response='Y'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from navigator_py.prompts import IsAskingAboutSpira\n",
    "\n",
    "is_about_spira = IsAskingAboutSpira(ai)\n",
    "(\n",
    "    is_about_spira.prompt(GOOD_QUESTION),\n",
    "    is_about_spira.prompt(BAD_QUESTION),\n",
    "    is_about_spira.prompt(GOOD_BUT_AMBIGUOUS_QUESTION),\n",
    "    is_about_spira.prompt(GOOD_BUT_AMBIGUOUS_QUESTION_2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_about_spira.prompt(GOOD_QUESTION).response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "db = duckdb.connect('./data/db.duckdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_data']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(db.sql('from upload where qa_id =1').df().to_dict(orient='records'), 'sample_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
